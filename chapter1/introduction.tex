%%% introduction
% plasticity vs evolution argument
Structural diversity and functional complexity of the human central nervous system is often placed into spotlight through the lens of evolution. While the sheer difference in cortex size might be the most striking difference between primate and other vertebrate brains, human brains stand out even more by its ability to invention and communicate symbol systems, which is poorly equipped in other primates. What special human brain functionality allowed us to invent cultural tools such as languages, numeric systems, and even arithmetics? As an exercise, one can consider two possible hypothesis at extreme ends of the structure-function argument:

\begin{itemize}
  \item Cultural functionality is acquired by expanded cortical plasticity unique to human brains, where newly acquired skills can be attributed to freshly established connections independent of an anatomical constraint.
  \item Humans have evolved specialized circuitry in the cortex, and specific brain circuitry contributes to unique cognitive functions. 
\end{itemize}

With the plastic brain scenario from  the first hypothesis, one can imagine if a group of people learned to play video games with novel control mechanisms, the plasticity induced changes in each person's brain may occur at various unpredictable locations in the brain. On the other hand, one would imply brain regions responsible for language and arithmetic are only found in humans. Of course, both of these extremes have been proven false by evidence. Specialists like musicians experience enhanced sensorimotor functions after repeated association, feedback, and cognitive practices, which are hallmarked by strengthened connections near the arcuate and intraparietal sulcus \cite{wan_music_2010}. Whereas mechanisms for vocal learning is identified in song birds \cite{warren_mechanisms_2011} and symbolic associations and computations in primate brains were also found to be associated with their prefrontal and parietal structures \cite{nieder_counting_2005,diester_semantic_2007}. If anatomical structures do support development of specialized functions in the brain, then how exactly does the complex connections made by billions of neurons give rise to the functional signals recorded by neuroscientists today? 

\section{Structure-Function Problem}
% Anatomy <--> function, link between neurobiology and observed phenomena 
Elucidating how structure shapes observable function is at the heart of a wide spectrum of scientific disciplines. Often, functional units have easily discernible structures governing their roles in a biological system. For example, the 3D molecular structure of a protein forming an ion channel receptor for transport across membranes. As the system becomes more complex, it becomes increasingly difficult to explain emergent function in relation to its underlying structure. Currently, the most complex physical system in the known universe is the central nervous system; the dense synaptic connections and staggering axonal projections in the brains of even simple organisms underlie the myriad of fascinating behaviors in nature.

% structure function history
The relationship between the brain's structure and function is of particular interest in neuroscience, and outside of the bench science laboratories, non-invasive imaging and physiological recordings have paved way for mathematical models relating biological structure to observed function. Hodgin and Huxley published the first successful model of ionic currents in 1952, simulating the action potential of neurons revealing behavior of ion channels and how types of ion channels can give rise to action potentials \cite{hodgkin_quantitative_1952}. Their work fueled experiments and simulations for the deciphering the diverse electrical activity in the central nervous system. Since then, new experimental techniques to measure neuronal activity at larger populations have become available for models of different scales, including optical recordings with voltage sensitive dyes over a cranial window, functional magnetic resonance imaging (fMRI), or high temporal resolution electro-encephalography (EEG) or magneto-encephalography (MEG) methods. These more powerful measurement techniques have revealed patterns of coherent activity that span larger volumes of the cortex, and with it, newer ideas of modeling coherent neural activity,

%as morphological variants of the nervous system have repeatedly been shown to be associated with behavioral changes due to the brain's organizations \cite{sharp_default_2011, shen_using_2017}. 

%A large body of work has been devoted to reproducing resting-state brain activity by means of computational modeling
%% Yeo's parcellation as example figure of clustering

\section{Mean Field Theory \& Dynamical Models}
Properties of neuronal spikes and mechanistic models of neuron firing tells a microscopic story of single neuronal unit decision making based on accumulation of evidence, but more complex behavior such as movement and cognition do not arise from the spikes of a single neuron. Instead, the collective behavior of many cortical, thalamic, and spinal neurons in a large nervous system network dictates behavior. But at this larger macroscopic scale, are there conceptual frameworks to mathematically model collective neural activity rather than mechanistic ion channels? Turns out in everyday life and most branches of science, there exists observed phenomena that reflects collective behavior and not that of individual units. Research in these areas is based on mathematical laws that govern macroscopic variables such as magnetic fields or fluid flow \cite{haken_introduction_2004,fiedler_coherent_1987}. These laws provide a framework for integrating, explaining, and predicting observed empirical data.

In neuroscience, there indeed exists mean field theories of electromagnetic brain activity \cite{PhysRevLett.77.960}. Rather than focusing on the ionic properties of an individual spiking neuron, such mean field models described the action of populations of neurons \cite{freeman_mass_1975}. With advanced computing power and larger data collectives, these models have been established to model seizures \cite{breakspear2006unifying}, encephalographies \cite{bojak2015emergence}, sleep \cite{phillips2007quantitative}, anesthesia \cite{bojak2005modeling}, and resting-state brain networks \cite{Honey2007,Deco2009}. Additionally, more effective approaches to model inversion; i.e. estimating likelihood of model parameters from data, make such models powerful tools for understanding perception, behavior, and multimodal neuroimaging data. 

The core concept behind mean field models are dynamical systems. First established by Newton and Leibniz, these differential equations describe how a variable changes as a function of the system's current states and parameters. The most famous example is Newton's second law $F = ma$, or more formally: $\frac{dV}{dt} = \frac{F}{m}$. Where $a$ is the acceleration and $V$ is the velocity of a mass $m$ under some force $F$. While Newton used such concepts to study planetary motion, dynamical systems expressing temporal dynamics of a system according to some underlying physical law can easily be translated to biology. For example, the Hodgin-Huxley model consist of membrane potential and ion channel variables, and differential equations are then derived from the biophysics of ion flow through voltage-gated channels, the conversion of the potential into firing rate, and other biophysical properties of neurons \cite{hodgkin_quantitative_1952,Wilson1972}. 

Single neuron spikes are highly nonlinear, and mean field approaches assume that coherent dynamics of neurons populations resemble that of single neurons. Accordingly, the mean ensemble activity is modeled with the same conductance-based theory as seen in single neuron spiking: the all-or-nothing firing of individual neurons is replaced by a sigmoid-shaped activation function that maps average membrane potential to mean firing rate. Such nonlinear systems of differential equations is complex and no clear analytical solution exists. Poincare had an insightful analysis of dynamical systems, illustrating the existence of a geometric phase space spanned by all of the system's variables \cite{poincare1899methodes}. A point in this space corresponds to unique combination of the system's states, yielding not only a time course for each of the variables, but also attractor behavior, bifurcation, and stability analysis. 

Attractors describe the characteristics of a dynamical system's activity: steady state, periodic, or chaotic. In terms of neuron dynamics, a periodically spiking neuron corresponds to a limit cycle attractor, but when a current input occurs, then chaotic oscillation is exhibited. While an attractor is structurally stable when a small change in parameters leads to slight changes in its shape, unstable attractors are found near bifurcation parameter values, where slight parameter changes drastically shifts dynamic behavior. Additionally, neurons are in constant presence of noise due to thermal energy, ion channel fluctuations, and irregular inputs from its connections \cite{faisal2008noise,mivsic2010brain}. Adding random noise to our dynamical system is the equivalent to adding perturbations at each time step, which may cause switching between attractor states. Figure \ref{fig:wc-pp} is an example Wilson-Cowan oscillator model (which we will see in more detail in Chapter 3)'s temporal dynamics and phase planes illustrated for different states, showing parameter regimes that produces steady states and limit cycle activity. 

\begin{figure}[htbp]
	\centering
	\includegraphics[width=\textwidth]{../figures/chapter1/wc-pp.png}
	\caption{Oscillatory Dynamics of Neural Mass Model}
	\caption*{Black lines are the traces of excitatory neuron population $E$ and red lines are those of inhibitory population $I$. Solid black and red lines are the average time courses computed over different brain regions. The $E-I$ plane trajectories are also shown for two stable states and a limit cycle state. Model parameters and simulations follow those of the \emph{Virtual Brain} defaults and as reviewed in \cite{sanz-leon_mathematical_2015}.}
	\label{fig:wc-pp}
\end{figure}

While the mathematics of stochastic differential equations is not trivial, modern large-scale brain dynamics are rooted in stochastic processes. The two key ingredients for differential equations are a coupling term that represents synaptic interactions between neurons, and a noise term to disrupt synchronization. The mathematical complexity of such models are one main reason why nonlinear dynamical systems have yet made their way into clinical settings. Another reason being mean field approaches view the brain dynamics as a forest while ignoring the behavior of individual trees and their leafs, meaning the inferred model parameters are a rough method to reduce the dimensionality of complex high dimensional data \cite{huys_computational_2016}. Interpreting model parameter estimates require robust methods to evaluate their values, as these estimates are sensitive to arbitrarily selected initial conditions and biases introduced by the objective function. Adding a network connections term and expanding the same dynamical system to all brain regions in a network further creates non-identifiability issues to model parameters. Chapter \ref{chap:nmm} will provide an overview of such a mean field model where we used a barebones approach to examine model performance and parameter interpretability, showing weaknesses in whole brain extensions of all-or-nothing nonlinear firing rates, and setting up analytical linear approaches to whole brain modeling in Chapter \ref{chap:lap} and \ref{chap:sgm}. 

\section{Graph Theory and Network Extensions}
% networks and graph theory in neuro modeling
Evidence of correlated activity is observed at the microscopic scale of communicating neurons, prompting extensive efforts to theoretically model these synchronous input and output relationships for decision making \cite{bogacz_physics_2006,hamburger_donald_1969,ratcliff_diffusion_2008,ratcliff_connectionist_1999}. While information from single neuron spike recordings can be sufficiently summarized with poisson distributions and probabilistic models \cite{smith_poisson_2010,zhang_optimal_2010}, high dimensional data from whole brain recordings with fMRI, EEG, and MEG require dimensionality reduction for meaningful interpretation. Graph theory and network theoretics have emerged as an advantagous tool in the field of neuroimaging.

The field of network neuroscience seeks to understand systems that are defined by nodes, or an individual functional unit, and their interactions through connections, which are often referred to as edges (see Figure \ref{fig:nodes-edges}). Together, these units and connections form a network where specific dynamics play out, building models that can explain, describe, or predict behavior of real physical networks. For example, pairwise interactions can be described using an $NxN$ adjacency matrix \cite{bollobas_graph_1979}, where $N$ is the number of nodes in the network, and each $ij$-th element of the matrix gives the connection strength between nodes $i$ and $j$. The properties of the adjacency matrix are characterized using a framework of mathematical approaches known as /emph{graph theory}. In neuroscience, the nodes can be chosen to reflect anatomical or functional units, such as cell bodies at the microscale or functionally distinct volumes at the macroscale. On the other hand, edges can be chosen to represent anatomical connections based on synapses, white matter streamlines or even statistical similarities in dynamics \cite{he_small-world_2007,varshney_structural_2011,bassett_conserved_2011,friston_functional_2011}. 

There are common measures of interest simply from the graph itself. For example, clustered connections can be used to identify locally efficient subgraphs or larger-scale "communities" that has specialized functions \cite{sporns_modular_2016}. Sparsely connected nodes can indicate "cavities" of architecture for information segregation \cite{sizemore_cliques_2018,reimann_cliques_2017}. Hubs are nodes which are more connected than usual, which can be localized or globally connected to form a "rich club" \cite{bassett_small-world_2006}. Such properties have already helped reveal how neurophysiological synchrony and information transmission is supported by specific nodes \& edge architectures \cite{borgers_synchronization_2003,ganmor_sparse_2011,avena-koenigsberger_communication_2017}. However, the works shown here will focus on an alternative application of graph theory: embedding data in a network and creating principled theory of how the system that generated the data might work \cite{abbott_theoretical_2008}. Such theory based models will combine a graph with a differential equation specifying dynamics of the nodes, edges, or both, granting the ability to make claims about the mechanism or dynamics that a data-driven network focused on empiricism cannot make. 

\begin{figure}[htbp]
	\centering
	\includegraphics[width=\textwidth]{../figures/chapter1/nodes_edges.png}
	\caption{Network schematics}
	\caption*{The simplest and most commonly used network model for neural systems is to represent pattern of connections (edges) between neural units (nodes). More sophisticated network models can be created from the simplest example shown here, such as adding edge weights and node values or explicit functions for dynamic. There are more complex hypergraphs and dynamically evolving graphs not shown in this example.}
	\label{fig:nodes-edges}
\end{figure}

Theory-based models commonly implement dynamic systems defined at each node. In neuroscience, the Hodgkin-Huxley model \cite{hodgkin_quantitative_1952} or the Rulkov map neurons \cite{zhu_effects_2016} are relevant at the neuronal microscopic scale. For populations of neurons, the FitzHugh-Nagumo \cite{acebron_noisy_2004} or Kuramoto oscillators \cite{breakspear_generative_2010} are all viable. For this work, we are focusing on whole brain MEG and fMRI data, therefore our nodes represent volumes of gray matter tissue. Mean field approach based models have been used in combination with edge adjacency matrices derived from white matter tractography for network dynamics \cite{breakspear_dynamic_2017}. At the whole-brain level, models must abstract away many anatomical details to ensure that the mathematics is manageable and the results are interpretable \cite{gerstner_theory_2012}. Additionally, at this macroscopic end of brain function, metrics such as functional connectivity do not necessarily have a physical counterpart, meaning laws of network models that capture the observed statistical functional patterns is more or less abstract or conceptual at best. This distinction between realism and phenomenology is important in determining whether a model can be used to infer functional patterns of realistic anatomy. 

\section{Summary}
Network extensions of mean field models have had a difficult time penetrating the barrier between research and practicality, and newer works in deep learning that ditches theory for empiricism is already outperforming these crude abstractions of neurophysiology in terms of speed and predictive power \cite{goncalves_training_2020,pervaiz_optimising_2020,parmar_spatiotemporal_2020}.  However, we might not have to surrender theory for full on data driven interpolations just yet. Spectral graph theory, which studies a graph in relationship to the characteristic polynomial, eigenvalues, and eigenvectors, have given rise to a family of whole brain models that utilizes far richer anatomical connection information. 

Chapter \ref{chap:dwi} will cover in detail how adjacency matrices, or "connectomes" are derived from diffusion weighted imaging (DWI), additionally showcasing advances made in image processing and white matter streamline generation. Chapter \ref{chap:nmm} will then examine how mean field approaches incorporate connectomes to model whole brain dynamics and discuss its shortcomings. Chapter \ref{chap:lap} will illustrate a simple use of spectral graph theory to attain spatial patterns of brain dynamics. Together with Chapter \ref{chap:sgm}, we showcase the rich amount of information that can be extracted from the spectral properties of brain networks. We believe the intricate organization of the brain has many specialized properties or functions we are yet unaware of, and the future is bright for careful combinations of experiment, modeling, and theory that will link increasingly realistic models with more neuroanatomical detail for analysis of observed neurophysiology. 
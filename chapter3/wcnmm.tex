\section{Introduction}
Computational models in neuroscience has advanced to untangle observed neural signals at the macroscopic scale, with fMRI being the most common modality for model based approaches to decipher complex neural mechanisms \cite{wilson_is_2015}). As nonlinear behavior is inevitably encountered at the microscopic scale of individual neurons, neural mass models (NMMs) have emerged as a powerful approach to balance interpretability and biological relevance of computational models. Such models summarizes the state of locally interacting neuron populations with few parameters and a conversion from mean excitation level to mean population response \cite{freeman_tutorial_1992}. The conversion is typically performed via a nonlinear sigmoid function, whereas the mean firing rates, connection profiles, and membrane potentials are parameterized mathematically to model the lumped activity of particular brain regions\cite{LopesdaSilva1974, robinson_prediction_2001, Valdes1999}. The Wilson-Cowan single oscillator model \cite{Wilson1972} have evolved into a family of macroscopic NMMs in recent literature; with derivations for neocortical dynamics \cite{cowan_wilsoncowan_2016}, controllability of brain networks \cite{muldoon_stimulation-based_2016}, biomarkers in disease \cite{Zimmermann2018}, and second order statistics of observed brain signals \cite{Deco2009, abeysuriya_biophysical_2018, singh_estimation_2020, byrne_next-generation_2019, wang_inversion_2019}.

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.75\textwidth]{../figures/chapter3/oscillator_unit.png}
    \caption{Illustration of a Wilson-Cowan oscillator unit.}
    \caption*{\textbf{A}: Local excitatory-inhibitory subpopulation structure with long range excitation only to the excitatory population. \textbf{B}: Oscillatory time course with default parameter settings as listed in Table \ref{tab:oscillator_parameters}, showing inhibitory population slightly lagging behind excitatory activity. \textbf{C}: Numerical simulation showing that the model is a single frequency oscillator, and at the current parameter regime, there exists a limit cycle stationary point at $E = 0.2$ and $ I = 0.1$. The number of stationary points and their behavior is subject to change depending on the initial conditions of the system.}
    \label{fig:unit}
\end{figure}

In a rare occurrence of public introspection amongst computational neuroscientists, Wilson and Niv's work \cite{wilson_is_2015} questioned whether model fitting is necessary for model-based analysis of fMRI. They addressed the weakness of models having free parameters, and the results of the analysis depend on how free parameters are set. While their work was limited to the context of reinforcement learning and a single learning rate parameter, their conclusion is generalizable to the wider model-based neuroscience field: precise identification of parameters is not always necessary, and it is hard to identify neural correlates with model-based analysis due to sensitivity to parameters. More recently, Hartoyo et al. \cite{hartoyo_parameter_2019} disseminated the problem of unidentifiabiliy in whole brain models, where different parameters combinations can generate similar model predictions, especially in higher order multi-parameter dynamical systems. It has long been known that fitting of an unidentifiable model to data results in large uncertainties, out of the 22 unknown parameters from the linearized network model implemented by Hartoyo et al., only one parameter was found to be identifiable when fitted to EEG data. Nonetheless, the computational neuroscience field pushed the limit of neuron population level mean field models and extended them to describe neural activity at the whole brain scale.

% Unsolved problem and why is this a problem? 
Despite the prolific use of NMMs for whole brain model-based analysis, their nonlinear nature leads to difficulties in parameter inference and generalizable decoding of neural mechanisms. The saddle point and Hopf bifurcation behavior of the Wilson-Cowan model are neatly described by the original work of Wilson \& Cowan \cite{Wilson1972}, the Virtual Brain \cite{sanz-leon_mathematical_2015} and reviewed by Breakspear \cite{breakspear_dynamic_2017}. Such nonlinear systems are silent or in steady-state for most parameter regimes, but interesting oscillating behavior is observed when network coupling or external driving force parameters push the system over the Hopf bifurcation. During parameter inference, constant switching between regimes and model behavior leads to many local minimas during parameter optimization. To avoid this non-convex problem and parameter identifiability issues, a common practice is to sets all biophysiological parameters for neural subpopulations in a NMM to be near an appropriate Hopf bifurcation point, then one or few global parameters are optimized to second order functional connectivity (FC) metrics such as brain regions pairwise correlation or synchrony \cite{Zimmermann2018, Deco2009, abeysuriya_biophysical_2018, wang_inversion_2019, demirtas_hierarchical_2019, honey_predicting_2009}. However, this leaves a few problems; What does manually selecting the initial parameter conditions imply about biological mechanisms? How does model optimization perform when inferring more than one global parameter at a time? Does the fMRI FC metric fitting translate well to the encephalography modalities? And finally whether the model parameters optimized for FC metrics translate well to the frequency spectrum?

% Variations of the network Wilson-Cowan model has been shown to decode neural signals by replicating functional connectivity metrics such as BOLD fMRI correlation, coupling, and phase lock metrics \cite{abeysuriya_biophysical_2018}. However, the set of parameters estimating second order statistical maps may not reproduce more direct neural derivatives such as the power spectrum density. Additionally, 

The traditional practice of only finding one or two optimal global connectivity related parameters, sometimes by manual grid search(see all works in Table \ref{tab:nmm_pubs}), while having its merits, is a fault of the high dimensional and nonlinear nature of NMMs. Techniques such as stochastic gradient descent found success in deep learning mainly due to the overparameterization of networks and the theory behind the universal approximation theorem \cite{lu_expressive_2017, zhou_universality_2020}. On the other hand, biophysiological models do not have the luxury of overparameterization, making any gradient descent algorithms difficult to implement for model optimization. In this work, we will investigate and discuss the difficulties faced in gradient descent based approaches to parameter inference, as well as address the short-comings of current network NMMs. We present a systematic examination of the Wilson-Cowan model. First, we examine the performance of a single Wilson-Cowan oscillator unit when fitting to broad band spectra, and whether MCMC sampling can converge to a reasonable posterior distribution for its parameters. In addition, we do not expect a single oscillator model to be able to capture the properties of an entire MEG frequency spectrum. However, if introducing coupling and delays to identical brain regions creates more dynamical oscillations as suggested by (\cite{Deco2009}), then a systematic examination of the new objective function space should be made. We implement the whole brain network Wilson-Cowan model to sample for posteriors that captures whole brain MEG derivatives such as coherence functional connectivity (COH) and amplitude envelope correlation (AEC), with the goal to verify whether the global parameters themselves can capture these metrics well, and whether the inferred parameters for FC produces a biophysiological power spectrum.

\begin{table}
 \caption{Whole Brain Neural Mass Model Parameter Inference Publications and their performance. This table does not include publications with whole brain mean field models or mechanistic models of neural activity. }
  \centering
  \begin{tabular}{llll}
    \toprule
    \cmidrule(r){1-2}
    Name & Modality  & Target  & Highest Accuracy \\
    \midrule
    Zimmerman et al. \cite{Zimmermann2018} & fMRI & Correlation FC & $r=0.57$  \\
    Honey et al. \cite{honey_predicting_2009} & fMRI & Correlation FC & $ r=0.48$ \\
    Demirtas et al. \cite{demirtas_hierarchical_2019} & fMRI & Correlation FC & $r=0.743$ \\
    Wang et al. \cite{wang_inversion_2019} & fMRI & Correlation FC & $r=0.46$ \\
    Schirner et al. \cite{schirner_inferring_2018} & fMRI & BOLD Time series & $r=0.50$ \\
    Falcon et al. \cite{falcon_virtual_2015} & fMRI  & Correlation FC  &  $r = 0.29$  \\
    Abeysuriya et al. \cite{abeysuriya_biophysical_2018} & MEG & Synchrony FC & $r = 0.48$ \\
    Deco et al. (2017) \cite{deco_single_2017}  & MEG & Envelope FC & $r=0.45$ \\
    Deco et al. (2009) \cite{Deco2009} & fMRI & Kuramoto order parameter & Not reported \\
    Hadida et al. \cite{hadida_bayesian_2018} & MEG & Envelope FC & $r=0.42$ \\
    \bottomrule
  \end{tabular}
  \label{tab:nmm_pubs}
\end{table}

\section{Methods}

\subsection{Experimental Procedure}

\subsubsection{Study Cohort}
We acquired MEG, anatomical MRI, and diffusion MRI for 36 healthy adult subjects (23 males, 13 females; 26 left-handed, 10 right-handed; mean age 21.75 years (range: 7–51 years). All study procedures were approved by the institutional review board at the University of California at San Francisco (UCSF) and are in accordance with the ethics standards of the Helsinki Declaration of 1975 as revised in 2008.

\subsubsection{MRI}
A 3 Tesla TIM Trio MR scanner (Siemens, Erlangen, Germany) was used to perform MRI using a 32-channel phased-array radiofrequency head coil. High-resolution MRI of each subject's brain was collected using an axial 3D magnetization prepared rapid-acquisition gradient-echo (MPRAGE) T1-weighted sequence (echo time [TE] = 1.64 ms, repetition time [TR] = 2,530 ms, TI = 1,200 ms, flip angle of 7°) with a 256-mm field of view (FOV), and 160 1.0-mm contiguous partitions at a 256×256 matrix. Whole-brain diffusion weighted images were collected at b = 1000s/mm2 with 30 directions using 2-mm voxel resolution in-plane and through-plane. The T1-weighted images were then parcellated into 68 cortical and 18 subcortical regions using the Desikan-Killiany atlas available in Freesurfer \cite{Fischl2012, Desikan2006}, the voxel labels were used to identify modeled dipoles in MEG source Reconstruction.

\subsubsection{Structural Connectivity Network}
To construct high resolution average connectivity matrices with the same Desikan-Killiany parcellations, we obtained openly available data from the Human Connectome Project \cite{McNab2013}. Subject specific structural connectivity was computed using diffusion MRI data: \emph{Bedpostx} was used to determine the orientation of brain fibers in conjunction witht \emph{flirt}, as implemented in the FSL software \cite{jenkinson_fsl_2012}. In order to determine the elements of the adjacency matrix, we performed tractography using \emph{probtrackx2}. We initiated 4,000 streamlines from each seed voxel corresponding to a cortical or subcortical gray matter structure and tracked how many of these streamlines reached a target gray matter structure. The weighted connection between the two structures $c_{j,k}$ was defined as the number of streamlines initiated by voxels in region $j$ that reach any voxels in region $k$, normalized by the sum of the source and target region volumes ($c_{j,k} = \frac{streamlines}{v_j + v_k}$). This normalization prevents large brain regions from having high connectivity simply due to having initiated or received many streamlines.

\begin{figure}[ht]
    \centering
    \includegraphics[width=\textwidth]{../figures/chapter3/oscillator_full_10hz.png}
    \caption{MCMC sampling of posterior distribution when maximizing likelihood to average MEG power spectrum.}
    \caption*{Corner plot of the posterior marginal distributions for parameters $\tau_e$, $\tau_i$, and $P$ when initiating from default values (\textbf{A}) or Hopf bifurcation point (\textbf{B}). \textbf{C} and \textbf{D} illustrates the average MEG spectrum (green) alongside the oscillator model spectra (blue) simulated with the posterior mean values shown from the corner plots above.}
    \label{fig:oscillator_full}
\end{figure}

\subsubsection{MEG Acquisition and Source Reconstruction}
MEG recordings were acquired at UCSF using a 275-channel CTF Omega 2000 whole-head MEG system from VSM MedTech (Coquitlam, BC, Canada). All subjects were instructed to keep their eyes closed for 5 min while their MEGs were recorded at a sampling frequency of 1,200 Hz. Then, all recordings were downsampled to 600 Hz and digitally filtered to remove DC offset and other noisy artifact outside of the 1 to 160Hz bandpass range prior to source reconstructions. To "invert" our sensor space recordings to the MRI voxels, we used an adaptive spatial filtering algorithm from the NUTMEG software tool \cite{dalal_nutmeg:_2004}. To prepare for source localization, all MEG sensor locations were co-registered to each subject's anatomical MRI scans. The lead field (forward model) for each subject was calculated in NUTMEG using a multiple local-spheres head model (three-orientation lead field) and an 8mm voxel grid which generated more than 5,000 dipole sources, all sources were normalized to have a norm of 1. Finally, the MEG recordings were projected into source space using a beamformer spatial filter. Source estimates tend to have a bias towards superficial currents and the estimates are more error-prone when we approach subcortical regions, therefore, only the sources belonging to the 68 cortical regions were selected to be averaged around the centroid.

\subsubsection{Functional Connectivity}
To analyze static functional connectivity in the MEG data and in the model, we computed both $\alpha$ band coherence (COH) and amplitude envelope correlation (AEC) with MNE Python's connectivity module implementations \cite{GramfortEtAl2013a}. First, multi-taper power spectrum densities were computed with digital prolate spheroidal sequences (DPSS) windows, and adaptive weights were used to combine the tapered spectra into full power spectral densities (PSDs). Finally for data with estimated cross- and power spectral densities $S_{xy}$ and $S_{xx}$, $S_{yy}$, the coherence is given by: 

\begin{equation}
    COH = \frac{|E[S_{xy}]|}{\sqrt{E[S_{xx}] * E[S_{yy}]}}
\end{equation}

To compute the amplitude envelope correlation, narrow band MEG time courses were obtained by bandpass filtering between 8 and 12 Hz. Hilbert transformed analytic signal corresponding to the orthogonalized narrow band time courses were computed to account for spatial leakage and zero-lag correlations. As recommended by \cite{hipp_large-scale_2012, brookes_measuring_2011, deco_single_2017}, MEG resting-state FC is maximized by solely considering ultra-slow fluctuations of the amplitude envelope. Therefore, a 4th order low-pass Butterworth filter at 0.2Hz is implemented, obtaining an envelope time course with alpha band carrier frequency, Finally, the Pearson correlation between every pair of envelope time courses were computed. 

\subsubsection{Model Parameter Inference}
Statistical sampling is a valuable tool in modern machine learning techniques, with Metropolis-Hasting \cite{hastings_monte_1970} and Gibbs \cite{geman_stochastic_1984, gelfand_sampling-based_1990} samplers being the most widely used algorithms. Such algorithms allow marginalization over parameters and find an estimate of the posterior density that is consistent with an observed dataset. The posterior probability function, is taken from Baye's rule and can be written as:

\begin{equation}
    p(\theta | y) \propto p(\theta) p(y | \theta)
\end{equation}

Where $p(\theta)$ is the distribution of prior beliefs on our model parameter values and $p(y | \theta)$ is a conditional likelihood probability distribution of observing our data given some set of model parameters $\theta$. To "marginalize" these parameters $\theta$ for a posterior probability distribution that is consistent with observed data, we write our conditional likelihood probability as a Gaussian:

\begin{equation}
    \log P(y | \theta) = - \frac{1}{2} \sum_n [\frac{y_n - f(E_n(t) - I_n(t))}{\sigma_n^2} + \log \sigma_n^2]
\end{equation}

For all parameters $\theta$, we will use uniform ("uninformative") priors for the term $p(\theta)$. In this formulation of MCMC, we are drawing samples from a probability distribution and we want that to be a probability distribution for parameters that produces some outcome $f(E_n(t) - I_n(t))$ which closely resembles the mean and variances of $y$. Here, we take $y$ and the function $f$ to be one of the MEG derived FC metrics.

\begin{table}[ht]
    \centering
    \caption{Default parameters for the Wilson-Cowan oscillator model. The excitatory driving force $P$ and time constants $\tau_e$ and $\tau_i$ are inferred during our experimentation, and their initial values are shown here. All default values are set such that small inputs would cause the system to oscillate (near the Hopf bifurcation point).}
    \begin{tabular}{lll}
    \toprule
    \cmidrule(r){1-2}
    Symbol                 &  Physiological Parameter     & Value \\
    \midrule
    $a_e$, $a_i$           &  Response function max slope & 1.3, 2 \\
    $\theta_e$, $\theta_i$ &  Response function threshold & 4, 3.7 \\
    $c_1$, $c_2$           &  Excitatory coupling         & 16, 12 \\
    $c_3$, $c_4$           &  Inhibitory coupling         & 15, 3  \\
    $r_e$, $r_i$           &  Refractory periods          & 1, 1 \\
    $\tau_e$, $\tau_i$     &  Time constants              & 8 ms \\
    $P$                    &  External driving force      & 1.25 \\
    \bottomrule
    \end{tabular}
    \label{tab:oscillator_parameters}
\end{table}

\section{The Wilson-Cowan Local Oscillator Model}
The original derivation of the single oscillator Wilson-Cowan model is illustrated in \cite{Wilson1972}, which observed cycles of excitation and inhibition producing oscillations typically seen observed in neural activity. Variants of this model has been implemented in several works, notably in (\cite{Deco2009}) with an extremely simplified model, to highlight the importance of delays and coupling in the brain. 

For local oscillations, two subpopulation of neurons are considered: an excitatory subpopulation ($E$) driving towards increased oscillatory activity, and an inhibitory subpopulation ($I$) driving towards quiescence. Given static local subpopulation couplings, varying firing amongst the two subpopulation of neurons, and an external driving force controlling the excitation, the model is able to describe the mean temporal dynamics of a mean field of neurons. Additionally, Wilson \& Cowan introduced "response functions" where local firing-thresholds within each subpopulation control the response of initially quiescent neurons to excitation, they are modeled as sigmoidal functions: 

\begin{equation}
\label{eq:sigmoid}
S(x) = \frac{1}{1+e^{-a(x-\theta)}} - \frac{1}{1+e^{a \theta}}
\end{equation}

Where $a$ and $\theta$ are parameters detailing the sigmoidal response function's maximum slope and the position of maximum slope respectively. Finally, with $E(t)$ and $I(t)$ representing the ratio of neurons firing for the two subpopulations respectively, the original Wilson-Cowan model is defined as:

\begin{equation}
    \label{eq:wco_ex}
\tau_e \frac{dE_{j}(t)}{dt} = -E_{j}(t) + (1 - r_e E_{j}(t)) \, S_e(c_1 E_{j}(t) - c_2 I_{j}(t) + P) + \epsilon_{j} (t)
\end{equation}

\begin{equation}
\label{eq:wco_in}
\tau_i \frac{dI_{j}(t)}{dt} = -I_{j}(t) + (1 - r_i I_{j}(t)) \, S_i(c_3 E_{j}(t) - c_4 I_{j}(t)) + \epsilon_{j} (t)
\end{equation}

Where $\tau_e$ and $\tau_i$ are time constants, the length of the refractory periods are parameterized by $r_e$ and $r_i$, $c_{1,2,3,4}$ are parameters representing the strength of excitatory-excitatory, inhibitory-excitatory, excitatory-inhibitory, and inhibitory-inhibitory connections respectively. $\epsilon_{j} (t)$ is the Gaussian noise, following Muldoon et al. \cite{muldoon_stimulation-based_2016}, the noise is scaled by $0.00001$ to ensure accuracy in step-wise integration of the differential equations. Lastly, $P$ is the external driving force pushing neurons out of quiescence. In this formulation, the time constants $\tau_e$ and $\tau_i$ together with the external drive parameter $P$ controls the Hopf bifucation point of this model, and all local connection strengths are held constant during inference. A summary of all model parameters is listed in Table \ref{tab:oscillator_parameters} and the model's time course and stationary points are illustrated in Figure \ref{fig:unit}, the parameter settings listed here is consistent with the limit cycle settings as published originally by Wilson \& Cowan \cite{Wilson1972} and the network controllability work by Muldoon et al. \cite{muldoon_stimulation-based_2016}.

\section{Oscillator Model Performance}

\begin{figure}[htbp]
    \centering
    \includegraphics[width=\textwidth]{../figures/chapter3/gridsearch_fullfit.png}
    \caption{Exhaustive grid search of the local oscillator model.}
    \caption*{Parameters affecting Hopf bifurcation point and oscillatory frequency were iteratively computed over a 3-dimensional grid to look for an optimal solution that matches closely with the observed average MEG spectra.}
    \label{fig:grid_search}
\end{figure}

Figure \ref{fig:oscillator_full} shows the oscillator model solutions sampled based on MCMC's walkers and maximum likelihood estimations. Here, we only focused on the three parameters affecting oscillatory frequency and Hopf bifurcation point, and implemented non-informative flat priors and arrived at posterior distributions that are moderately constrained by the observed average MEG spectrum. We initiated our samplers under two conditions: default parameter values as described in \cite{muldoon_stimulation-based_2016} ($\tau_{e/i} = 8$ and $P=1.25$) or near a 10Hz oscillatory Hopf bifurcation point ($\tau_e = 3.0$, $\tau_i = 3.6$, and $P=1.1$). Under both scenarios, we see high uncertainly for both $\tau_i$ and $P$, despite $\tau_e$ having a narrower distribution, the samplers for of both $\tau_e$ and $\tau_i$ did not accept parameters far away from their initial positions. Moreover, $\tau_i$ seems to have more than one highly probable value. Using the posterior means as model parameter inputs, the simulation produced a power spectrum with 15Hz peak and subsequent harmonic peaks at 30 and 45 Hz when initiated from default values. While the most notable peak in a human MEG spectrum is the alpha peak around 10Hz, the likelihood maximization mechanism arrived at a solution that captured the 10Hz alpha peak only when initiated near a 10Hz oscillatory regime. We will discuss the question of MCMC sampler convergence in the discussion section.

States of quietness, transience, steady-state, and limit cycle in dynamical systems used by NMMs are clearly illustrated and reviewed in (\cite{breakspear_dynamic_2017}) and (\cite{sanz-leon_mathematical_2015}), where small changes in model parameters or initial conditions can affect the behavior of the model output. Due to these changes in model behavior, it is extremely easy for our samplers to be stuck in a certain regime and reject samples that are outside of the preset limit cycle regime as set by default parameters. Figure \ref{fig:grid_search} shows the 6 most closely matching spectras found by exhaustive grid search. These results suggest that there exists sets of parameters that can produce oscillations at any of the notable frequency peaks, but also bringing into question the problem of identifiability. We see two sets of parameters producing notable peaks at 20Hz, and a set of parameter producing a 15Hz oscillation that is different from the set obtained from MCMC sampling. 

\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{../figures/chapter3/oscillator_periodic_10hz.png}
   \caption{MCMC sampling of posterior distribution when maximizing likelihood to average periodic power spectrum.}
    \caption*{Corner plot of the posterior marginal distributions for parameters $\tau_e$, $\tau_i$, and $P$ when initiating from default values (\textbf{A}) or Hopf bifurcation point (\textbf{B}). \textbf{C} and \textbf{D} illustrates the average MEG spectrum (green) alongside the oscillator model spectra (blue) simulated with the posterior mean values shown from the corner plots above.}
    \label{fig:oscillator_periodic}
\end{figure}

A broad band frequency spectrum, however, has both a periodic and aperiodic component ($\frac{1}{f}$-like fall off), and interpretations of physiological spectrum can be compromised by periodic features mixed with offsets and exponential components that do not contribute to frequency specific feataures \cite{donoghue_parameterizing_2020}. Paired with the fact that our oscillator model can only produce one principle frequency of oscillation, we implemented the \emph{FOOOF} algorithm from Donoghue et al. \cite{donoghue_parameterizing_2020} to obtain a periodic frequency power spectrum. Figure \ref{fig:oscillator_periodic} shows the same posterior sampling procedure's results when maximizing likelihood to a periodic component only spectrum, and the bottom panels show both the average MEG and model simulated spectrum without the aperiodic component. In this new maximum likelihood space and sampling from the same initial starting ranges, we see that the samplers were able to explore much wider parameter ranges and did not get stuck in the initial regime. However, the uncertainties around the posterior means is still large, and parameters $\tau_i$ and $P$ even shows hints of bimodal distributions. Lastly, despite a smoother surface for the samplers, the final posterior means did not produce a spectra that closely matched with the most notable frequency peaks in the observed MEG spectrum for both initial conditions. 

\section{Network Extension of Wilson-Cowan Model}

\begin{figure}
    \centering
    \includegraphics[width=0.75\textwidth]{../figures/chapter3/meg_metrics.png}
   \caption{Source localized MEG functional connectivity matrics.}
    \caption*{MEG recordings were source localized and averaged to each cortical brain region as parcellated by the Desikan-Killiany atlas \cite{Desikan2006}, $\alpha$ band coherence matrix of pairwise cortical brain regions showing hemispheric block structure is on the left, and narrowband amplitude envelope correlation matrix showing weaker hemispheric block structure is on the right.}
    \label{fig:meg_metrics}
\end{figure}

The oscillator model is a single frequency oscillator model, to extend this to the whole brain network, we are simply saying the broadband oscillations we observe in encephalography recordings are fully dependent on the reverberant properties in the long range white matter connections. On the macroscopic whole brain scale, the excitatory neuronal population receives delayed signals from interconnected regions via white matter streamlines and coupling terms, thus extending the original oscillator model to a network of $j$ brain nodes coupled by a connectivity matrix $A$:

\begin{equation}
    \label{eq:wcn_ex}
\tau_e \frac{dE_{j}(t)}{dt} = -E_{j}(t) + (1 - r_e E_{j}(t)) \, S_e(c_1 E_{j}(t) - c_2 I_{j}(t) + c_5 \sum_{k} A_{jk} E_{k}(t - \tau_d^k) + P) + \epsilon_{j} (t)
\end{equation}

\begin{equation}
\label{eq:wcn_in}
\tau_i \frac{dI_{j}(t)}{dt} = -I_{j}(t) + (1 - r_i I_{j}(t)) \, S_i(c_3 E_{j}(t) - c_4 I_{j}(t)) + \epsilon_{j} (t)
\end{equation}

Here, the delay parameter $\tau_d^k$ accounts for propagation time between distance brain regions as calculated by white matter streamline distances, which interferes with the dynamics of each node. To oscillate, the model is pushed passed the Hopf bifurcation point by either $P$ or a high enough global coupling strength  ($c_5$). Therefore,  like all pre-existing literature, only the global parameters that affect the dynamics in the default local parameter regime are inferred. 

In the whole brain network model, the simulation is hindered by the feedback \& coupling term at each time step. In \cite{hadida_bayesian_2018}, this problem was simplified by simulating without noise and an average propagation delay. However, they still encountered quadratic increases in complexity and doubling in computation time. Specifically, at each time step size $h$, the sum of delay terms in each equation needs to be computed at time $t$ and $t+h$, and interpolated for each substep. This substep size is subject to further decrease if a noise term is introduced to the equations, which further increases the amount of computations necessary for one simulation of the model. Additionally, initialization of delayed nonlinear system of equations is a sensitive operation. Delayed systems require a smooth function for initialization to be defined over $[t_0 - \tau_{max} t_0]$, where $t_0$ is the initial time and $\tau_{max}$ is the largest delay. Additionally, it should itself be a solution of the system, which makes this circular and impossible. 

There is no solution to this problem of increasing complexity and initialization, in our simulations, and consistent with prior works \cite{hadida_bayesian_2018}, we set the initial function to be constant and nodes are initially disconnected from the network for a period of time equal to maximum delay. Additionally in our implementation, we interpolate past solutions for all unique delays in our network and the full solution with an adaptive-step Runge-Kutta integrater of order 8 \cite{hairer_analysis_2008}.


\begin{figure}[htbp]
    \centering
    \includegraphics[width=\textwidth]{../figures/chapter3/network_c5_fits.png}
    \caption{Network Wilson Cowan model sampling results for one representative subject.}
    \caption*{Top row shows the results for coherence maximum likelihood sampling, broadband AEC sampling in the middle, and bottom row shows the results for narrowband AEC maximum likelihood sampling. Left column shows the sampled posterior distributions for the global coupling parameter $c_5$, the middle column shows the network model simulated FC matrices with the posterior mean, and right column shows the scatter plot and linear regression line for the FC matrix entries. Both parameter posteriors achieved near zero slope for the linear fit.}
    \label{fig:networkc5}
\end{figure}

\subsection{Network Model Performance}
Existing literatures expanding neural mass models to the network level fixes the local within population connection strength parameters (in this case $c_1$, $c_2$, $c_3$, and $c_4$) for all nodes in the network. Meaning all oscillators at every node of the network are identical, the only thing differentiating and producing varying dynamics is driven by the connection properties. In particular, the connection strengths scaling each node's output, the delayed inputs to each node, and the noise being propagated throughout the network can augment oscillatory activity at each node. For fMRI modeling, a Balloon-Windkessel hemodynamic model \cite{buxton_dynamics_1998, friston_nonlinear_2000} is usually layered on top of the NMMs for parameter optimization (For example, see \cite{Zimmermann2018}). On the other hand, encephalography based modeling wraps another metric on top of the NMM outputs, such as kuramoto coupling parameters in \cite{Deco2009}. Here, we examine purely the NMM outputs and whether they are good basis for network modeling. We fixed the time constants $\tau_e$ and $\tau_i$ so that each oscillator has a base oscillating frequency of near 10Hz, and we fixed $P = 1.1$ to be right below the Hopf bifucation point so the network's dynamics is purely driven by connectivity. 

Figure \ref{fig:meg_metrics} shows the source localized MEG derived metrics for the cortical regions parcellated according to the Desikan-Killiany atlas \cite{Desikan2006}. Coherence is a purely spectral content based second order statistic, whereas AEC is a time course transformation based correlation metric. These two FC metrics covers both aspects of a neural time series recording; synchrony and delayed correlations between connected regions. Here we focus on the $\alpha$ band coherence and AEC between 8Hz and 12Hz, showing hemispheric block structure, where intra-hemispheric regions experience high coherence while inter-hemispheric regions experience low coherence.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=\textwidth]{../figures/chapter3/individual_c5_spectra.png}
    \caption{Comparison between MEG spectra and network model simulated spectra for 4 subjects.}
    \caption*{Model simulated spectra were computed with the posterior mean for global coupling parameter $c_5$ after MCMC sampling. The average spectra from all cortical brain regions is shown as lines, and the standard deviation summarized from all regions is shown as shaded fillings. Source localized MEG spectra is shown in blue, coherence parameter simulated spectra is shown in red, narrowband and broadband AEC parameter simulated spectra are shown in black and green respectively. All spectras were normalized by their minimum and maximum values to be between 0 and 1 for equal visualization scale.}
    \label{fig:c5_spectra}
\end{figure}

We first sampled only the global coupling parameter $c_5$'s posterior distribution with MCMC. Figure \ref{fig:networkc5} shows the posteriors for likelihood functions attempting to maximize both COH (top) and AEC (middle, bottom). While both sampling attempts arrived at posteriors with relatively low certainty, the resulting FC metrics do not fit well with observed MEG derived FC. The simulated coherence FC did not preserve the hemispheric block structure, and lost the sparsity we saw in Figure \ref{fig:meg_metrics}'s coherence FC. On the other hand, both narrowband and broadband AEC preserved the sparsity but did not perform well in terms of correlation as shown by the scatter plot. 

At its essence, FC matrices are second order statistics computed from time course activity over time, whether recordings like MEG or simulated activity with our network NMM. And here we see that changing the maximum likelihood definition or objective function in parameter inference drastically affects the gradients and eventual posterior parameter values. For modeling based analysis to claim its parameters describe the full relationship between some phenomenon and brain activity, we believe the model needs to encapsulate both first order statistics as well as its second order metrics. In Figure \ref{fig:c5_spectra}, we show the MEG spectra in comparison to the model spectra simulated with posterior parameter means obtained by attempting to match both COH and AEC. We see that introducing a connectome with delays and noise to the model did expand the oscillatory repertoire of the oscillators, and because we fixated our time constants to produce oscillations of 10Hz, the resulting spectra with newly sampled global coupling parameter also produces spectra with an alpha peak. Interestingly, narrowband AEC samplers were consistently stuck around $c_5 \approx 1.5$, and produced nearly  identical spectra with oscillations in the $\beta$ range only. On the other hand, other simulated spectra are not able to produce an isolated $\beta$ peak near 20Hz, which is also a significant characteristic of a human brain signal power spectrum. Furthermore, Figure \ref{fig:c5_spectra} shows that $c_5$ sampling of the network model only arrived at two different ranges of posterior means when fitting to FC; Coherence and narrowband AEC fitting led to a lower estimated global coupling value, $c_5 \approx 1.6$ produced nearly identical spectra with narrow variance and a leading spectral peak around 10Hz whereas $c_5 \approx 1.5$ produced the black colored spectra with oscillations around 25Hz. Secondly, fitting to AEC (green) led to a higher estimated global coupling of $c_5 \approx 9$, as illustrated by the widely varying green spectra at lower magnitude. However, one subject (lower left) fell into a minima near $c_5 \approx 1.6$ when fitting to AEC, thus producing an identical spectrum to COH fitting, underlining the issue of parameter unidentifiability and unfriendly landscape of model fitting.

Next, in contradiction to what's commonly done in the field, we initialize all global parameters $tau_e$, $\tau_i$, $P$, and $c_5$ for MCMC sampling. We wanted to see if increasing the dimensionality of the parameter space can improve model performance. Additionally, we set the initial conditions for each $\tau_e$ and $\tau_i$ samplers to be $8$ms with a small random noise, this way, the model is initialized away from an optimal limit cycle regime. In this new initialization parameter regime, the network model produces a uniform 20Hz oscillatory at every node. We want to check if the MCMC samplers can find parameter posteriors that are in the appropriate oscillatory regime as opposed to being stuck traversing between oscillatory, quiescence, and transience activity regimes. 

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.82\textwidth]{../figures/chapter3/coh_results.png}
    \caption{Global Parameters Sampling for maximizing likelihood to $\alpha$ band coherence.}
    \caption*{Corner plot constructed after MCMC sampling for 50000 steps. The first 5000 steps were discarded and final posterior distribution shown were thinned by half. Neither $\tau_e$ or $\tau_i$ deviated far from the initial positions, and there is high uncertainty around $c_5$ and $P$. Bottom row shows the model simulated coherence with resulting posterior means and the scatter plot against MEG coherence with linear fit for the coherence matrix entries showing no correlation at all.}
    \label{fig:coh_hopf}
\end{figure}

Figure \ref{fig:coh_hopf} and Figure \ref{fig:aec_hopf} illustrates the sampler's inability to survey the 4 dimensional parameter space set by the network model for alpha band coherence or AEC. In Figure \ref{fig:coh_hopf}, neither $\tau_e$ or $\tau_i$ samplers escaped their initial positions near 8ms, and the samplers were stuck between a few acceptable ranges. Additionally, global coupling parameter $c_5$ and external driving force $P$ had high uncertainties as well. As mentioned above, the initialization parameters puts the model into a 20Hz oscillatory regime, and despite an objective function set for likelihood of the $\alpha$ band, the samplers were not able to arrive at a viable solution for $\alpha$ band coherence. The simulated coherence shown in Figure \ref{fig:coh_hopf} in combination with the spectra shown in Figure \ref{fig:psd_hopf} shows that there was no oscillations or variability near 10Hz. Additionally, not only were the samplers stuck in the initial parameter ranges of producing a 20Hz oscillation, one subject (lower right) transitioned into a transient state with no oscillatory activity. 

Similarly, Figure \ref{fig:aec_hopf} shows the same failure of samplers escaping its initial positions. However, in this case, the posterior distributions for all parameters had extremely narrow 95\% confidence intervals. Additionally, we do not see these posteriors having more than one frequently visited value as compared to coherence and Figure \ref{fig:coh_hopf}. Despite this certainty, the simulated AEC matrix had very little to non-significant correlation and linear fit. And the elements of the simulated AEC matrix is much higher than the observed MEG AEC values. This high AEC is an obvious result when taking into account the quiet spectra shown in Figure \ref{fig:psd_hopf}, the AEC parameters produced no oscillatory activity in the alpha range and were stuck in the oscillatory regime of the initial parameter conditions near 20Hz. For two of the subjects, the spectra in green showed little to no variability and no obvious speaks, indicating all nodes of the network were in a transient state. Lastly, the spectra produced by parameters sampled for alpha band AEC were also stuck in the initial ranges, producing spectra almost identical to coherence or broadband AEC fits. 

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.82\textwidth]{../figures/chapter3/aec_results.png}
    \caption{Global Parameters Sampling for maximizing likelihood to amplitude envelope correlation.}
    \caption*{Corner plot constructed after MCMC sampling for 25000 steps. The first 5000 steps were discarded and final posterior distribution shown were thinned by half. Neither $\tau_e$ or $\tau_i$ deviated far from the initial positions, but the posterior distributions have narrower confidence intervals as compared to coherence sampling in Figure \ref{fig:coh_hopf}. Bottom row shows the model simulated AEC with resulting posterior means and the scatter plot against MEG AEC with linear fit showing a slight correspondence, but different scales.}
    \label{fig:aec_hopf}
\end{figure}

\begin{figure}[htbp]
    \centering
    \includegraphics[width=\textwidth]{../figures/chapter3/individual_hopf_spectra.png}
   \caption{Comparison between MEG spectra and posterior mean global parameters simulated spectra for 4 subjects.}
    \caption*{Model simulated spectra were computed with the posterior means for ${\tau_e, \tau_i, c_5, P}$ after MCMC sampling. The average spectra from all cortical brain regions is shown as lines, and the standard deviation summarized from all regions is shown as shaded fillings. Source localized MEG spectra is shown in blue, coherence parameters simulated spectra is shown in red, narrowband and broadbanc AEC parameters simulated spectra is shown in black and green respectively. All spectras were normalized by their minimum and maximum values to be between 0 and 1 for equal visualization scale.}
    \label{fig:psd_hopf}
\end{figure}

Based on the existing literature's avoidance of this parameter fitting procedure for network NMMs, and the fact that we are implementing a barebones approach with no adjustments to the mean firing rate model, we did not expect the network NMM to perform well in capturing neural activity statistics. Typically, a more specified model is required to produce any meaningful statistics. We've shown that a typical oscillator unit produces limit cycle activity at a single frequency as specified by the model parameters, and adding a network component to the model does not easily translate to network metrics being produced by the network model. Furthermore, the existing practice of setting model parameters near an oscillatory regime and searching for parameters that fits some neural activity metric well doesn't take into account the problem of unidentifiable parameters. Lastly, network models are high in dimensionality, making parameter posterior sampling a consuming and unpredictable exercise. And by limiting every node to have identical parameter values to decrease parameter inference complexity is not biologically realistic and produces irrelevant results. 


\subsection{NMM Based Analysis}
Parameter inference of a full brain network neural mass model is rarely attempted in the computational neuroscience field. The complexity due to their nonlinearity and network size with numerous delays, combined with the tangled nature of neurological recordings, makes the optimization of these NMMs a nearly impossible task. The first thing we've shown here is that an oscillating Wilson-Cowan neural population has a limit cycle activity for a specific frequency given a specific set of parameters. Extending this model to the network scale by introducing delays and connectivity, but fixing all nodes to have identical parameters for computational tractability does not necessarily provide the broadband spectrum that is characteristic of observed encephalography recordings.

In most NMM analysis (see Table \ref{tab:nmm_pubs}), the comparisons are made to fMRI BOLD functional connectivity, which has limited temporal resolution, and recorded neural dynamics are limited to below 1Hz. Whereas NMMs mean firing rate outputs are simulated with millisecond time-steps, to compensate for the mismatch in data type and temporal resolution, a Balloon-Windkessel hemodynamics model \cite{friston_nonlinear_2000, deco_resting-state_2013} is used to transform the model outputs to BOLD signals (For example, see \cite{deco_resting-state_2013, deco_how_2014}) for functional connectivity comparison. Practically, parameters for the NMM is first fitted either by exhaustive grid search or constrained optimization, followed by another model fitting for the BOLD signal transformation. The two layers or model fitting leads to low interpretability of final parameters, and low correlations with high variances in parameter distribution on the group level leads to uncertainties about any result. 

In other publications, for the sake of computational tractability, a more simplistic Wilson Cowan model variant with more varied model behavior is used as an alternative (For example, see \cite{Deco2009}, where an additional Kuramoto coupling parameter is required for spectrum computation). In this model, the sigmoidal activation function is simplified to one term, and its inputs are generalized to include network level driving forces and noise. Bringing into question the meaning behind a nonlinear sub-population neuron activation function for network level inputs. In Muldoon et al.'s work on network controllability \cite{muldoon_stimulation-based_2016}, the original network model was made to oscillate at 20Hz, and no model fitting was performed, only changes in functional network properties based on a single nodal stimulation with the driving force parameter was measured. Again, based on our results with the same model implementation and identical parameter regimes, a network of identical oscillators will not provide simulated functional network metrics that closely resemble those of a realistic brain. Functional connectivity metrics and network activity will undoubtedly change according to network connections, but the parameters producing the accepted functional outputs may not produce an acceptable spectra. Moreover, it is unclear if there is an optimal parameter regime that is suitable for a given brain state due to parameter unidentifiability issues as discussed by numerous works \cite{hartoyo_parameter_2019, chis_relationship_2016, villaverde_observability_2019}.

Overall, it is no surprise that the promise of personalized diagnosis with model based analysis has been overtaken by the capabilities of deep learning or the analytical efficiencies of linear models \cite{Becker2018, raj_spectral_2020}. Deep learning neural networks takes advantage of repeated and chained linear regression units with rectified linear activation units (ReLu) to make extremely fast classifications of data, and it has already been shown to produce high accuracy for biological phenomenon \cite{pulvermuller_biological_2021, parmar_spatiotemporal_2020}. In this framework, interpolating high dimensional data for a given classification or embedding is made possible by the massive flexibility of layered linear units, and is already superior than any nonlinear NMMs in terms of training speed and prediction power. In fact, finding a global minimum to the massive parameter space of deep neural networks is frowned upon due to generalizability, and training of complex parameter spaces is made efficient by backpropagation of gradients. On the other hand, Linear models such as the works cited above transforms dynamical systems to the Fourier domain and uses structural eigen basis to relate functional patterns to a given structural connectome. These efforts provide models that are low-dimensisonal, hierarchical across spatial scales, and efficient to compute as the analytical models do not require step adaptive integrators. The trade offs over linear and nonlinear macroscopic models was recently examined by Nozari et al. \cite{nozari_is_2020}, whom also reported on the performance of nonlinear NMMs to fit neurodynamic time course data. In addition to reporting linear models outperforming nonlinear models on the macroscopic scale, their work also commented on the macroscopic brain properties that masks nonlinear dynamics. Together with our results, we believe it is evident that nonlinear NMMs are not the optimal model of choice for model-based macroscopic neurophysiological data analysis.

\subsection{MCMC Convergence \& Other Approaches}
In our results, we avoided the topic of convergence for our samplers. Convergence is often a difficult topic to discuss for MCMC sampling based analysis, because convergence can only be guaranteed for sample statistical problems with a high number of samples. For high dimensional model parameter inference, quality of the sampled posterior is often evaluated by acceptance ratio of samples and inspection of the posterior as compared to the prior. Additionally, statistics like the Gelman-Rubin statistic \cite{gelman_inference_1992} are not applicable to modern MCMC algorithms as the multi-samplers used for parallelization are not independent from each other. On the other hand, integrated autocorrelation time \cite{goodman_ensemble_2010} quantifies the effective number of independent samples needed and sampler efficiency. However, our sampling chains for all scenarios produced high acceptance rates and integrated autocorrelation times that required extremely large amounts of MCMC samples and weeks long computation times. This implies that the log likelihood ratios conditioning our parameter inference problem is not sufficently exploring the entirety of the parameter space. Our resulting weak fits with the fact that majority of accepted samples have low likelihoods suggest global sampling methods are not the proper approach for NMM parameter optimization. Secondly, the Wilson-Cowan network model parameters are not capable of describing the high dimensional features of functional neural recordings.

One may suggest that global parameter optimization methods with multiple local searches may improve our model fits. Approaches such as simulated annealing \cite{Kirkpatrick1983} or basin hopping \cite{Wales1997} may arrive at inferred parameters that provide better fits, but has their short comings. Firstly, these approaches provide point estimates of model parameters, ignoring any uncertainty in the case of complex parameter spaces that may have more than one possible solutions. In addition, while convergence for these algorithms can be determined by consecutive visits to the current  optimal solution, the solution is still sensitive to initialization, and there is no formal way of selecting optimization hyper-parameters such as tolerances, iterations, temperature, or cooling schedule (exploration scheme). Furthermore, these approaches do not provide any formalism for an objective function, and does not have the advantage of using a Bayesian framework which provides a set framework and a model evidence metric. 


\subsection{Dynamic Causal Modeling}
One popular model based analysis is dynamic causal modeling (DCM), these approaches focus on discovery of effective network connectivity, and the emphasis is placed on a few nodes of interest \cite{friston_network_2011}. Typically used for determining causality in fMRI data, this method aims to find the causal network that best supports the observed data as measured by Bayesian model evidence \cite{stephan_nonlinear_2008}. While the goal of DCM based studies are different from model based analysis of neural recordings, the insights gained from causality in small subnetworks of the brain with a formal framework is extremely valuable. However, this approach is made impractical for whole brain studies due to the number of evaluations and possible networks required to be considered. Furthermore, in the case of NMMs, the nonlinear nature of the model makes the model inversion problem extremely inefficient. Whereas DCM works typically takes advantage of fully connected network models that are easily invertable. 

\subsection{Variational Inference}
Variational inference is a statistical method used to approximate parameter posterior densities in Bayesian models, where parameters and their uncertainties are obtained through optimization rather than expensive sampling. The optimization is theoretically performed based on the gradients of a posterior distribution obtained through Baye's theorem. However, the partial derivatives culminating in the full parameter space gradient is intractable for macroscopic network NMMs, and current implementations of variational inference such as \emph{Edward2} \cite{tran2018simple} and \emph{PyMC3} \cite{Salvatier2016} uses a "black box" inference approach. While these approaches seem effective for simpler problems, the need to fully sample the posterior hinders the usage of these tools for network NMMs. In particular, a network model such as the Wilson Cowan model requires the integration of two sets of derivatives for every brain region. And additionally for each brain region, the algorithm is required to remember past solutions at all nodes, and interpolate the past solutions based on unique distance induced delays as assigned by the connectomes. This makes computation of one realization with minimal time steps extremely slow on CPU backends (several minutes), needing to sample thousands of solutions for the likelihood estimation per iteration is simply impossible when taking into account computing time. Moreover, efficient parameter optmization relies on convex structural properties of the problem to guarantee rapid convergence to a solution. But in the case of NMMs and their black-box objective functions, these properties cannot be theoretically determined. In our case of expensive objective functions, the strategy needs to restrict the exploration of the search space to remain computationally tractable, further crippling network model based approaches to exploring neural data.

\subsection{Challenges and Limitations}
We've already addressed the problem of MCMC requiring large numbers of samples for diversity and statistical validity, which is computationally expensive and often times not practical to achieve with network scale NMMs. To further add to the challenges, reliable estimations of functional connectivity maps require on the order of a minute worth of data. This finite data problem in source localized encephalography recordings is comprehensively explored by Sommariva et al. \cite{sommariva_comparative_2019}, concluding unsurprisingly that estimating functional connectivity is prone to errors with less data. Our numerical network NMM simulations of 2 seconds is on the low end of data length, and is certainly prone to unreliable functional connectivity outputs. This effect is further amplified if initialization of the delayed dynamical systems are not properly selected, as initial values affect the stationary points and stability of the oscillating system. We limited our simulation time to 2 seconds for the simple reason of practical computation time. Numerical integration of NMMs require time-steps below the millisecond to ensure accuracy at every time step. When more uncertainties from delayed inputs and noises at every network node is introduced, the integrators usually takes even smaller time steps, exponentially increasing the time needed to simulate one realization of the NMM. Therefore, parameter exploration poses immediate impossibilities, which is often commented on by published NMM works or avoided by simplifying the problem. 

\subsection{Conclusion}
The dimensionality and nonlinearity of network NMMs leads to low confidence for a given system to produce functional dynamics with desired properties, and subsequently compare these models for effective analysis. Additionally, there is no formal methodology for selecting initial values of the delayed dynamics or the initial positions of parameters. In the case of the Wilson-Cowan model, the mean firing rates of neuron sub-populations output is simply not appropriate for encephalography modality data modeling. Recordings of neural activity involves excitation driven currents or blood-oxygen dependent activity, the sensor level average activity we observe in data is not simply explained by firing rate models and more mechanistic details are required. In our work, we show that one of the widely adapted NMMs is specified for neuron population level firing rate studies of a specified frequency, extending such a model to the network scale does not allow accurate, reliable, or efficient optimization. Furthermore, second order connectivity statistics often does not encapsulate sufficient evidence for tuning of network model parameters. Overall, we believe claims of unidentifiable network NMM parameters as low dimensional descriptors of biophysiological data should be carefully examined. More effective alternatives in linear models and deep learning has already surpassed NMMs, and offers more insight into the embeddings of functional brain recordings.